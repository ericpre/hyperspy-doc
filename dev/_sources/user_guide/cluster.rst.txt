

Cluster analysis
================

.. versionadded:: 1.6

Introduction
------------

`Cluster analysis <https://en.wikipedia.org/wiki/Cluster_analysis>`_ or clustering 
is the task of grouping a set of measurements such that measurements in the same 
group (called a cluster) are more similar (in some sense) to each other than to 
those in other groups (clusters).
A Hyperspy signal can represent a number of large arrays of different measurements
which can represent spectra, images or sets of paramaters.
Identifying and extracting trends from large datasets is often difficult and 
PCA, BSS, NMF and cluster analysis play an important role in this process. 

Cluster analysis, in essence, compares the "distances" (or similar metric) 
between different sets of measurements and groups those that are closest together.   
The features it groups can be raw data points, for example, comparing for 
every navigation dimension all points of a spectrum. However if the 
dataset is large the process of clustering can be computationally intensive so 
clustering is more commonly used on an extracted set of features or parameters.
For example, extraction of two peak positions of interest via a fitting process
rather than clustering all spectra points.

Decomposition or Blind Source Separation (PCA, NMF, BSS etc.) can produce a smaller set 
of features as it reduces the description of the data into a set of loadings and factors. 
The loadings capture a core representation of the features in the data and the factors 
provide the mixing ratios of these loadings that best describe the original data. 
Overall this represents a much smaller data volume compared to the original data 
and can helps to identify initial differences. Cluster analysis is then performed 
on the factors. 

A detailed description of the application of cluster analysis in x-ray
spectro-microscopy and further details on the theory and implementation can be found here.  
:ref:`[Lerotic2004] <Lerotic2004>`.

Nomenclature
------------

Taking the example of a 1D Signal of dimensions `(20, 10|4)` containing the
dataset, we say there are 200 *samples*. The four measured parameters are the
*features*. If we choose to search for 3 clusters within this dataset, we
derive two main values: the `labels`, of dimensions `(20, 10|3)` (each
sample is assigned a label to each cluster), and the `centers`, of
dimensions `(3, 4)` (each centre has a coordinate in each feature).
If you take all features within a given cluster and average them
this average set of features is the center of that cluster.


Cluster analysis in HyperSpy
----------------------------

All HyperSpy signals have the following methods to perform cluster analysis:

* :py:meth:`~.learn.mva.MVA.cluster_analysis`
* :py:meth:`~.signal.MVATools.plot_cluster_results`
* :py:meth:`~.signal.MVATools.estimate_number_of_clusters`



The clustering itself uses either the ``kmeans`` or ``agglomerative``
methods from `sklearn.clustering <https://scikit-learn.org/stable/modules/clustering.html>`_
and further details on operation and implementation can be found there

Pre-processing
--------------

Cluster analysis measures the distances between features and groups them. It
is often necessary to pre-process the features in order to obtain meaningful
results.

For example, consider the case of a spectrum image. If the signal magnitude
varies strongly from spectra to spectra or image to image, then the
clustering would group the data into clusters based on differences in
magnitude. However, if the objective is to identify, for example, that peak 1
is the same as peak 2, then this magnitude variation needs to be removed and
the spectra should therefore all be normalized first to remove the effect of
peak height.

As discussed previously, decomposition methods decompose data into a set of
components and a set of factors defining the mixing needed to represent the
data. If signal 1 is reduced to three components with mixing 0.1 0.5 2.0 and
signal 2 is reduced to a mixing of 0.2 1.0 4.0 it should be clear that these
represent the same signal but with a scaling difference. Normalization of the
data can again be used to remove scaling effects.

Pre-processing to remove scaling effects is handled within the cluster
analysis methods and implements the ``standard`` , ``minmax`` or ``norm``
pre-processing methods as standard or allows for custom methods from
`sklearn.preprocessing
<https://scikit-learn.org/stable/modules/preprocessing.html>`_ For the
reasons described above the scaling will influence the results and should be
evaluated for the problem under investigation. Briefly, ``norm`` treats the
features as a vector and normalizes the vector length. This is the default
scaling used in the cluster analysis methods. ``standard`` re-scales each
feature by removing the mean and scaling to unit variance. ``minmax``
normalizes each feature between the minimum and maximum range of that
feature.


Cluster Centers and Labels
--------------------------

The cluster labels and centers found using the sklearn methods are based on 
the features after they've been whitened or scaled.  
To create a meaningful representation of the clusters areas with identical label
are averaged to create a set of cluster centers. This averaging can be performed
on the  ``signal`` itself, the  ``bss``  or  ``decomposition`` results or a
user supplied signal.


Examples 
--------

We can use the `make_blobs <https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html>`_
supplied by `scikit-learn` to make dummy data to see how clustering might work in practice.
 
.. code-block:: python

    >>> import hyperspy.api as hs
    >>> from sklearn.datasets import make_blobs
    >>> data = make_blobs(
    >>>         n_samples=500,
    >>>         n_features=4,
    >>>         shuffle=False)[0].reshape(50, 10, 4)
    >>> s = hs.signals.Signal1D(data)

make_blobs creates 3 distinct centres or 3 "types" of signal by default. 
If we examine the signal using PCA we can see that there are 3 regions but
their interpretation of the signal is a little ambigous.  

To see how cluster analysis works it's best to first examine the signal.
Moving around the image you should be able to see 3 distinct regions in which
the 1D signal modulates slightly.  

.. code-block:: python

    >>> s.plot()


If we then perform PCA we start to see the 3 regions a little more clearly but
the factors and loadings don't match up with the original 1D signals or image.

.. code-block:: python

    >>> s.decomposition()
    >>> s.plot_decomposition_results()


We can then cluster, using the decomposition results, to find similar regions
and the representative features in those regions. 
This indentifies 3 regions and the average or representative 1D signals in 
those regions

.. code-block:: python

    >>> s.cluster_analysis(3)
    >>> s.plot_cluster_results("decomposition")
    


To see what the labels the cluster algorithm has assigned you can inspect:

.. code-block:: python

    >>> s.learning_results.cluster_membership


These are split into a cluster_labels array to help plotting and masking:

.. code-block:: python

    >>> s.learning_results.cluster_labels


The clustering methods currently supported in hyperspy are kmeans and 
agglomerative. Additional keywords can be passed directly to the scikit learn 
methods in the following manner:


.. code-block:: python

    >>> s.cluster_analysis("decomposition",n_clusters=3,
    >>>        algorithm='agglomerative',
    >>>        kwargs={affinity='cosine', linkage='average'})
    >>> s.plot_cluster_results()


Estimating the number of clusters
---------------------------------

In this case we know there are 3 signals but for real examples it is difficult
to define the number of clusters to use. A number of metrics, such as elbow, 
Silhouette and Gap can be used to determine the optimal number of clusters. 
The elbow method measures the sum-of-squares of the distances within a 
cluster and as for the PCA decomposition an "elbow" or point where the gains 
diminish with increasing number of clusters indicates the ideal number of 
clusters. Silhouette analysis measures how well separated clusters are and 
can be used to determine the most likely number of clusters. As the scoring 
is a measure of separation of clusters a number of solutions may occur and 
maxima in the scores are used to indicate possible solutions. Gap analysis
is similar but compares the “gap” between the clustered data results and 
those from a randomly data set of the same size. The largest gap indicates 
the best clustering. The metric results can be plotted to check how 
well-defined the clustering is.

.. code-block:: python

    >>> s.evaluate_number_of_clusters("decomposition",metric="gap")
    >>> s.plot_cluster_metric()
    
The optimal number of clusters can be set or accessed from the learning 
results

.. code-block:: python

    >>> s.learning_results.number_of_clusters
    
If running cluster analysis and the number of clusters have not been
specified the algorithm will attempt to use the estimated number of clusters

.. code-block:: python

    >>> s.cluster_analysis()


Clustering different signal information
---------------------------------------

As discussed in the introduction, clustering can be performed on fitted or
extracted parameters. Given an existing fitted model the parameters 
can be extracted as signals and stacked. Decomposition and clustering can then 
be applied as described previously to identify trends in the
fitted results.

.. code-block:: python

    >>> import hyperspy.misc.utils.stack
    >>> # create a signal called original signal and setup fitting
    >>> # model created using two gaussians and fitting performed... 
    >>> fitted_centre1 = g1.centre.as_signal()
    >>> fitted_centre2 = g2.centre.as_signal()
    >>> new_signal = stack([fitted_centre1,fitted_centre2]])
    >>> new_signal.cluster_analysis("signal",source_for_centers=original_signal)
    
To extract cluster centers based on the decomposition results
of a signal using the results of fitting on that signal 

.. code-block:: python

    >>> import hyperspy.misc.utils.stack
    >>> # A signal called original signal and setup fitting
    >>> # model created using two gaussians and fitting performed... 
    >>> fitted_centre1 = g1.centre.as_signal()
    >>> fitted_centre2 = g2.centre.as_signal()
    >>> new_signal = stack([fitted_centre1,fitted_centre2]])
    >>> original_signal.cluster_analysis(new_signal,source_for_centers="decomposition")






